{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ariaberlian/rbm_sr/blob/main/rbm_sr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A1niMNjgRwd2"
   },
   "outputs": [],
   "source": [
    "# Python v3.6.12\n",
    "\n",
    "##### Run This to Install #####\n",
    "\n",
    "# ! pip install requirements.txt\n",
    "\n",
    "## !git clone https://github.com/albertbup/deep-belief-network.git\n",
    "## !pip install -r \"deep-belief-network/requirements.txt\"\n",
    "## !mv \"deep-belief-network\" \"deep_belief_network\"\n",
    "\n",
    "## \"\"\"\n",
    "## - add this to dbn/tensorflow/models.py:\n",
    "##     import tensorflow._api.v2.compat.v1 as tf\n",
    "##     tf.disable_v2_behavior()\n",
    "## \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9E9apHoc70M",
    "outputId": "3b957557-10ce-46ea-8301-b79e3c096a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:19: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from deep_belief_network.dbn.tensorflow.models import UnsupervisedDBN # use \"from dbn.tensorflow import SupervisedDBNClassification\" for computations on TensorFlow\n",
    "from deep_belief_network.dbn.tensorflow.models import SupervisedDBNRegression\n",
    "from utils.data_processing import DataProcessing\n",
    "from utils.image_file_util import *\n",
    "from utils.scoring import *\n",
    "from utils.visualizer import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import multiprocessing\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LV1RtFIhUdV"
   },
   "source": [
    "## DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i, row, model_name, model, interpolation_factor, patch_size, stride,data_train_size, dp: DataProcessing):\n",
    "\n",
    "    test_var = row[f\"test{i}\"]\n",
    "    test_ref_var = row[f\"test{i}_ref\"]\n",
    "    test_image = load_image(f\"test/{test_var}\")\n",
    "    test_reference_image = load_image(f\"test/{test_ref_var}\")\n",
    "    model = model.load(f\"model/{model_name}.h5\")\n",
    "    \n",
    "    print(f\"Testing model: {model_name}...\")\n",
    "    print(f\"Test Image: {test_var}\")\n",
    "    print(f\"Reference Image: {test_ref_var}\")\n",
    "    interpolated_test = dp.interpolate(test_image, interpolation_factor)\n",
    "\n",
    "    test_patches = dp.get_patches(interpolated_test, patch_size, stride)\n",
    "    norm, int_test_dct_ex = dp.normalize_for_rbm(test_patches[:test_patches.shape[0]])\n",
    "    if data_train_size < test_patches.shape[0]:\n",
    "        norm = dp.normalize_for_rbm(test_patches[:data_train_size])\n",
    "\n",
    "    test_patches_flat = dp.preprocess_for_rbm(norm)\n",
    "    norm=None\n",
    "\n",
    "    result_flat = model.predict(test_patches_flat)\n",
    "    result_flat = dp.proccess_output(test_patches_flat, result_flat, interpolation_factor)\n",
    "\n",
    "    result_patches, recons_dct_ex = dp.inverse_preprocess(\n",
    "        result_flat, (patch_size[0], patch_size[1], 3)\n",
    "    )\n",
    "\n",
    "    reconstruct_image = dp.reconstruct_from_patches(\n",
    "        result_patches, original_shape=test_reference_image.shape, patch_size=patch_size, stride=stride)\n",
    "    \n",
    "    # visualize4image(test_image, \"Test Image\", interpolated_test, \"Interpolated Test Image\",\n",
    "    #                  reconstruct_image, \"Reconstructed Image\", test_reference_image, \"Reference Image\")\n",
    "    \n",
    "    # visualize4Histogram(test_image, \"Test Image\", interpolated_test, \"Interpolated Test Image\",\n",
    "    #                  reconstruct_image, \"Reconstructed Image\", test_reference_image, \"Reference Image\")\n",
    "    \n",
    "    # visualize_dct(int_test_dct_ex, \"Interpolated Test\")\n",
    "    # visualize_dct(recons_dct_ex, \"Reconstructed Image\")\n",
    "\n",
    "    norm, refs_dct_ex = dp.normalize_for_rbm(dp.get_patches(test_reference_image,patch_size,stride))\n",
    "    # visualize_dct(refs_dct_ex, \"Reference Image\")\n",
    "\n",
    "    refs_flat = dp.preprocess_for_rbm(norm)\n",
    "\n",
    "    psnr_baseline = calculate_psnr(test_reference_image, interpolated_test)\n",
    "    ssim_baseline = calculate_ssim(test_reference_image, interpolated_test)*100\n",
    "    rmse_baseline = calculate_rmse(refs_flat, test_patches_flat)\n",
    "    psnr = calculate_psnr(test_reference_image, reconstruct_image)\n",
    "    ssim = calculate_ssim(test_reference_image, reconstruct_image)*100\n",
    "    rmse = calculate_rmse(refs_flat,result_flat)\n",
    "\n",
    "    print(f\"PSNR {i} Interpolated: {psnr_baseline:,.3f} dB\")\n",
    "    print(f\"SSIM {i} Interpolated: {ssim_baseline:,.3f} %\")\n",
    "    print(f\"RMSE {i} Interpolated: {rmse_baseline}\")\n",
    "    print(f\"PSNR {i}: {psnr:,.3f} dB\")\n",
    "    print(f\"SSIM {i}: {ssim:,.3f} %\")\n",
    "    print(f\"RMSE {i}: {rmse}\")\n",
    "    \n",
    "    # Cleanup: explicitly delete all variables\n",
    "    del test_image, test_reference_image, interpolated_test, test_patches, test_patches_flat\n",
    "    del result_flat, result_patches, reconstruct_image, refs_flat, norm\n",
    "    gc.collect()\n",
    "\n",
    "    return psnr_baseline,ssim_baseline,psnr,ssim,rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file Excel\n",
    "excel_name = \"Experimentation.xlsx\"\n",
    "exp_df = pd.read_excel(excel_name, engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001\n",
      "Starting training for model: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001\n",
      "[START] Training step:\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:146: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:180: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:186: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:187: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:215: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:114: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 6.791897\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 6.425733\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 6.000698\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5.752247\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.491747\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5.249385\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5.099263\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 4.956623\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4.891418\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 4.822236\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 4.769548\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 4.731515\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 4.725952\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 4.688595\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 4.670674\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 4.672697\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 4.666114\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 4.658415\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 4.654625\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 4.649905\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 4.649227\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 4.649298\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 4.657007\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 4.654033\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 4.652563\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 4.652913\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 4.657643\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 4.657705\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 4.656235\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 4.657042\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 4.658237\n",
      "Training stopped at epoch: 31 because of no improvement\n",
      "Best RBM Reconstruction error: 4.649227032847094\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.626055\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.722888\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.761742\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.774504\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.748037\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.734875\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.720215\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.676223\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.667445\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.657509\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.620149\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.601782\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.564549\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.542538\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.545736\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.491270\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.474777\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.451767\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.410459\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.389585\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.348230\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.342763\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.307397\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.288507\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.265027\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.258038\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.226992\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.217568\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.189485\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.182281\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 0.157912\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 0.147889\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 0.139957\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 0.132610\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 0.111254\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 0.111615\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 0.107817\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 0.093610\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 0.096050\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 0.089751\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 0.081055\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 0.073825\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 0.067732\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 0.061769\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 0.064111\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 0.057538\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 0.059546\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 0.058845\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 0.054501\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 0.052204\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 0.050901\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 0.049732\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 0.048355\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 0.046696\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 0.044792\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 0.047116\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 0.043240\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 0.041957\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 0.044698\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 0.041588\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 0.040715\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 0.040593\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 0.042877\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 0.041382\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 0.040395\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 0.040713\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 0.039921\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 0.039152\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 0.039940\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 0.038702\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 0.038601\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 0.038587\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 0.037506\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 0.038044\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 0.038282\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 0.038038\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 0.037441\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 0.036859\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 0.037078\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 0.038773\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 0.036967\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 0.036550\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 0.037464\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 0.037719\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 0.037162\n",
      ">> Epoch 86 finished \tRBM Reconstruction error 0.036675\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 0.036869\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 0.036793\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 0.036524\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 0.036473\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 0.036396\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 0.036340\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 0.036701\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 0.036480\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 0.036607\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 0.036945\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 0.036705\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 0.036463\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 0.036597\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 0.035817\n",
      ">> Epoch 101 finished \tRBM Reconstruction error 0.036572\n",
      ">> Epoch 102 finished \tRBM Reconstruction error 0.035922\n",
      ">> Epoch 103 finished \tRBM Reconstruction error 0.036297\n",
      ">> Epoch 104 finished \tRBM Reconstruction error 0.036321\n",
      ">> Epoch 105 finished \tRBM Reconstruction error 0.036360\n",
      ">> Epoch 106 finished \tRBM Reconstruction error 0.035925\n",
      ">> Epoch 107 finished \tRBM Reconstruction error 0.036238\n",
      ">> Epoch 108 finished \tRBM Reconstruction error 0.035677\n",
      ">> Epoch 109 finished \tRBM Reconstruction error 0.036016\n",
      ">> Epoch 110 finished \tRBM Reconstruction error 0.036143\n",
      ">> Epoch 111 finished \tRBM Reconstruction error 0.036459\n",
      ">> Epoch 112 finished \tRBM Reconstruction error 0.035731\n",
      ">> Epoch 113 finished \tRBM Reconstruction error 0.034947\n",
      ">> Epoch 114 finished \tRBM Reconstruction error 0.036031\n",
      ">> Epoch 115 finished \tRBM Reconstruction error 0.035645\n",
      ">> Epoch 116 finished \tRBM Reconstruction error 0.035653\n",
      ">> Epoch 117 finished \tRBM Reconstruction error 0.034579\n",
      ">> Epoch 118 finished \tRBM Reconstruction error 0.035459\n",
      ">> Epoch 119 finished \tRBM Reconstruction error 0.035502\n",
      ">> Epoch 120 finished \tRBM Reconstruction error 0.035622\n",
      ">> Epoch 121 finished \tRBM Reconstruction error 0.035599\n",
      ">> Epoch 122 finished \tRBM Reconstruction error 0.035641\n",
      ">> Epoch 123 finished \tRBM Reconstruction error 0.035091\n",
      ">> Epoch 124 finished \tRBM Reconstruction error 0.035505\n",
      ">> Epoch 125 finished \tRBM Reconstruction error 0.035136\n",
      ">> Epoch 126 finished \tRBM Reconstruction error 0.035389\n",
      ">> Epoch 127 finished \tRBM Reconstruction error 0.035670\n",
      "Training stopped at epoch: 127 because of no improvement\n",
      "Best RBM Reconstruction error: 0.0345793142914772\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 4.411865\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 3.959484\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 3.404845\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 2.753479\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2.160722\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 1.715234\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1.319229\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.970985\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.718648\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.570674\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.418760\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.312468\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.240419\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.203228\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.154682\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.108169\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.088736\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.077359\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.058275\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.049067\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.039862\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.028210\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.025534\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.019821\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.019601\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.012826\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.016674\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.011193\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.010120\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.009191\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 0.010238\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 0.008542\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 0.007807\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 0.006457\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 0.007168\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 0.005389\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 0.006139\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 0.005437\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 0.004567\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 0.004120\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 0.003754\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 0.004253\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 0.004674\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 0.004006\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 0.003523\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 0.004016\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 0.004397\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 0.004405\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 0.003377\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 0.003497\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 0.003518\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 0.004016\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 0.003608\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 0.003983\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 0.004162\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 0.003998\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 0.003545\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 0.003377\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 0.003624\n",
      "Training stopped at epoch: 59 because of no improvement\n",
      "Best RBM Reconstruction error: 0.0033766694832593203\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.675525\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.795286\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.904801\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.968347\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.988494\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 1.001585\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.970919\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.962491\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.927559\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.922409\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.893783\n",
      "Training stopped at epoch: 11 because of no improvement\n",
      "Best RBM Reconstruction error: 0.675525426864624\n",
      "[END] Training step\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:393: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/biofis/Downloads/rbm_sr/deep_belief_network/dbn/tensorflow/models.py:412: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.475266\n",
      ">> Epoch 1 finished \tANN training loss 0.465641\n",
      ">> Epoch 2 finished \tANN training loss 0.456231\n",
      ">> Epoch 3 finished \tANN training loss 0.447029\n",
      ">> Epoch 4 finished \tANN training loss 0.438031\n",
      ">> Epoch 5 finished \tANN training loss 0.429231\n",
      ">> Epoch 6 finished \tANN training loss 0.420625\n",
      ">> Epoch 7 finished \tANN training loss 0.412206\n",
      ">> Epoch 8 finished \tANN training loss 0.403971\n",
      ">> Epoch 9 finished \tANN training loss 0.395914\n",
      ">> Epoch 10 finished \tANN training loss 0.388031\n",
      ">> Epoch 11 finished \tANN training loss 0.380318\n",
      ">> Epoch 12 finished \tANN training loss 0.372771\n",
      ">> Epoch 13 finished \tANN training loss 0.365385\n",
      ">> Epoch 14 finished \tANN training loss 0.358157\n",
      ">> Epoch 15 finished \tANN training loss 0.351082\n",
      ">> Epoch 16 finished \tANN training loss 0.344157\n",
      ">> Epoch 17 finished \tANN training loss 0.337378\n",
      ">> Epoch 18 finished \tANN training loss 0.330742\n",
      ">> Epoch 19 finished \tANN training loss 0.324245\n",
      ">> Epoch 20 finished \tANN training loss 0.317884\n",
      ">> Epoch 21 finished \tANN training loss 0.311656\n",
      ">> Epoch 22 finished \tANN training loss 0.305557\n",
      ">> Epoch 23 finished \tANN training loss 0.299585\n",
      ">> Epoch 24 finished \tANN training loss 0.293736\n",
      ">> Epoch 25 finished \tANN training loss 0.288009\n",
      ">> Epoch 26 finished \tANN training loss 0.282399\n",
      ">> Epoch 27 finished \tANN training loss 0.276905\n",
      ">> Epoch 28 finished \tANN training loss 0.271523\n",
      ">> Epoch 29 finished \tANN training loss 0.266252\n",
      ">> Epoch 30 finished \tANN training loss 0.261088\n",
      ">> Epoch 31 finished \tANN training loss 0.256029\n",
      ">> Epoch 32 finished \tANN training loss 0.251073\n",
      ">> Epoch 33 finished \tANN training loss 0.246217\n",
      ">> Epoch 34 finished \tANN training loss 0.241460\n",
      ">> Epoch 35 finished \tANN training loss 0.236799\n",
      ">> Epoch 36 finished \tANN training loss 0.232232\n",
      ">> Epoch 37 finished \tANN training loss 0.227757\n",
      ">> Epoch 38 finished \tANN training loss 0.223372\n",
      ">> Epoch 39 finished \tANN training loss 0.219075\n",
      ">> Epoch 40 finished \tANN training loss 0.214864\n",
      ">> Epoch 41 finished \tANN training loss 0.210737\n",
      ">> Epoch 42 finished \tANN training loss 0.206693\n",
      ">> Epoch 43 finished \tANN training loss 0.202729\n",
      ">> Epoch 44 finished \tANN training loss 0.198844\n",
      ">> Epoch 45 finished \tANN training loss 0.195037\n",
      ">> Epoch 46 finished \tANN training loss 0.191305\n",
      ">> Epoch 47 finished \tANN training loss 0.187647\n",
      ">> Epoch 48 finished \tANN training loss 0.184061\n",
      ">> Epoch 49 finished \tANN training loss 0.180547\n",
      ">> Epoch 50 finished \tANN training loss 0.177102\n",
      ">> Epoch 51 finished \tANN training loss 0.173725\n",
      ">> Epoch 52 finished \tANN training loss 0.170414\n",
      ">> Epoch 53 finished \tANN training loss 0.167169\n",
      ">> Epoch 54 finished \tANN training loss 0.163987\n",
      ">> Epoch 55 finished \tANN training loss 0.160869\n",
      ">> Epoch 56 finished \tANN training loss 0.157811\n",
      ">> Epoch 57 finished \tANN training loss 0.154813\n",
      ">> Epoch 58 finished \tANN training loss 0.151875\n",
      ">> Epoch 59 finished \tANN training loss 0.148993\n",
      ">> Epoch 60 finished \tANN training loss 0.146169\n",
      ">> Epoch 61 finished \tANN training loss 0.143399\n",
      ">> Epoch 62 finished \tANN training loss 0.140684\n",
      ">> Epoch 63 finished \tANN training loss 0.138021\n",
      ">> Epoch 64 finished \tANN training loss 0.135411\n",
      ">> Epoch 65 finished \tANN training loss 0.132852\n",
      ">> Epoch 66 finished \tANN training loss 0.130342\n",
      ">> Epoch 67 finished \tANN training loss 0.127881\n",
      ">> Epoch 68 finished \tANN training loss 0.125469\n",
      ">> Epoch 69 finished \tANN training loss 0.123103\n",
      ">> Epoch 70 finished \tANN training loss 0.120783\n",
      ">> Epoch 71 finished \tANN training loss 0.118509\n",
      ">> Epoch 72 finished \tANN training loss 0.116278\n",
      ">> Epoch 73 finished \tANN training loss 0.114092\n",
      ">> Epoch 74 finished \tANN training loss 0.111947\n",
      ">> Epoch 75 finished \tANN training loss 0.109844\n",
      ">> Epoch 76 finished \tANN training loss 0.107782\n",
      ">> Epoch 77 finished \tANN training loss 0.105760\n",
      ">> Epoch 78 finished \tANN training loss 0.103778\n",
      ">> Epoch 79 finished \tANN training loss 0.101833\n",
      ">> Epoch 80 finished \tANN training loss 0.099927\n",
      ">> Epoch 81 finished \tANN training loss 0.098057\n",
      ">> Epoch 82 finished \tANN training loss 0.096224\n",
      ">> Epoch 83 finished \tANN training loss 0.094426\n",
      ">> Epoch 84 finished \tANN training loss 0.092663\n",
      ">> Epoch 85 finished \tANN training loss 0.090934\n",
      ">> Epoch 86 finished \tANN training loss 0.089239\n",
      ">> Epoch 87 finished \tANN training loss 0.087577\n",
      ">> Epoch 88 finished \tANN training loss 0.085946\n",
      ">> Epoch 89 finished \tANN training loss 0.084348\n",
      ">> Epoch 90 finished \tANN training loss 0.082780\n",
      ">> Epoch 91 finished \tANN training loss 0.081243\n",
      ">> Epoch 92 finished \tANN training loss 0.079735\n",
      ">> Epoch 93 finished \tANN training loss 0.078257\n",
      ">> Epoch 94 finished \tANN training loss 0.076807\n",
      ">> Epoch 95 finished \tANN training loss 0.075386\n",
      ">> Epoch 96 finished \tANN training loss 0.073992\n",
      ">> Epoch 97 finished \tANN training loss 0.072625\n",
      ">> Epoch 98 finished \tANN training loss 0.071284\n",
      ">> Epoch 99 finished \tANN training loss 0.069969\n",
      ">> Epoch 100 finished \tANN training loss 0.068680\n",
      ">> Epoch 101 finished \tANN training loss 0.067416\n",
      ">> Epoch 102 finished \tANN training loss 0.066176\n",
      ">> Epoch 103 finished \tANN training loss 0.064961\n",
      ">> Epoch 104 finished \tANN training loss 0.063769\n",
      ">> Epoch 105 finished \tANN training loss 0.062600\n",
      ">> Epoch 106 finished \tANN training loss 0.061454\n",
      ">> Epoch 107 finished \tANN training loss 0.060330\n",
      ">> Epoch 108 finished \tANN training loss 0.059227\n",
      ">> Epoch 109 finished \tANN training loss 0.058146\n",
      ">> Epoch 110 finished \tANN training loss 0.057087\n",
      ">> Epoch 111 finished \tANN training loss 0.056047\n",
      ">> Epoch 112 finished \tANN training loss 0.055028\n",
      ">> Epoch 113 finished \tANN training loss 0.054029\n",
      ">> Epoch 114 finished \tANN training loss 0.053049\n",
      ">> Epoch 115 finished \tANN training loss 0.052088\n",
      ">> Epoch 116 finished \tANN training loss 0.051146\n",
      ">> Epoch 117 finished \tANN training loss 0.050222\n",
      ">> Epoch 118 finished \tANN training loss 0.049316\n",
      ">> Epoch 119 finished \tANN training loss 0.048427\n",
      ">> Epoch 120 finished \tANN training loss 0.047556\n",
      ">> Epoch 121 finished \tANN training loss 0.046702\n",
      ">> Epoch 122 finished \tANN training loss 0.045865\n",
      ">> Epoch 123 finished \tANN training loss 0.045044\n",
      ">> Epoch 124 finished \tANN training loss 0.044238\n",
      ">> Epoch 125 finished \tANN training loss 0.043449\n",
      ">> Epoch 126 finished \tANN training loss 0.042675\n",
      ">> Epoch 127 finished \tANN training loss 0.041916\n",
      ">> Epoch 128 finished \tANN training loss 0.041171\n",
      ">> Epoch 129 finished \tANN training loss 0.040442\n",
      ">> Epoch 130 finished \tANN training loss 0.039726\n",
      ">> Epoch 131 finished \tANN training loss 0.039024\n",
      ">> Epoch 132 finished \tANN training loss 0.038337\n",
      ">> Epoch 133 finished \tANN training loss 0.037662\n",
      ">> Epoch 134 finished \tANN training loss 0.037001\n",
      ">> Epoch 135 finished \tANN training loss 0.036352\n",
      ">> Epoch 136 finished \tANN training loss 0.035717\n",
      ">> Epoch 137 finished \tANN training loss 0.035093\n",
      ">> Epoch 138 finished \tANN training loss 0.034482\n",
      ">> Epoch 139 finished \tANN training loss 0.033883\n",
      ">> Epoch 140 finished \tANN training loss 0.033296\n",
      ">> Epoch 141 finished \tANN training loss 0.032720\n",
      ">> Epoch 142 finished \tANN training loss 0.032155\n",
      ">> Epoch 143 finished \tANN training loss 0.031601\n",
      ">> Epoch 144 finished \tANN training loss 0.031059\n",
      ">> Epoch 145 finished \tANN training loss 0.030527\n",
      ">> Epoch 146 finished \tANN training loss 0.030005\n",
      ">> Epoch 147 finished \tANN training loss 0.029493\n",
      ">> Epoch 148 finished \tANN training loss 0.028992\n",
      ">> Epoch 149 finished \tANN training loss 0.028500\n",
      ">> Epoch 150 finished \tANN training loss 0.028018\n",
      ">> Epoch 151 finished \tANN training loss 0.027546\n",
      ">> Epoch 152 finished \tANN training loss 0.027083\n",
      ">> Epoch 153 finished \tANN training loss 0.026629\n",
      ">> Epoch 154 finished \tANN training loss 0.026183\n",
      ">> Epoch 155 finished \tANN training loss 0.025747\n",
      ">> Epoch 156 finished \tANN training loss 0.025319\n",
      ">> Epoch 157 finished \tANN training loss 0.024900\n",
      ">> Epoch 158 finished \tANN training loss 0.024489\n",
      ">> Epoch 159 finished \tANN training loss 0.024085\n",
      ">> Epoch 160 finished \tANN training loss 0.023690\n",
      ">> Epoch 161 finished \tANN training loss 0.023303\n",
      ">> Epoch 162 finished \tANN training loss 0.022923\n",
      ">> Epoch 163 finished \tANN training loss 0.022551\n",
      ">> Epoch 164 finished \tANN training loss 0.022186\n",
      ">> Epoch 165 finished \tANN training loss 0.021828\n",
      ">> Epoch 166 finished \tANN training loss 0.021478\n",
      ">> Epoch 167 finished \tANN training loss 0.021134\n",
      ">> Epoch 168 finished \tANN training loss 0.020797\n",
      ">> Epoch 169 finished \tANN training loss 0.020467\n",
      ">> Epoch 170 finished \tANN training loss 0.020143\n",
      ">> Epoch 171 finished \tANN training loss 0.019826\n",
      ">> Epoch 172 finished \tANN training loss 0.019515\n",
      ">> Epoch 173 finished \tANN training loss 0.019210\n",
      ">> Epoch 174 finished \tANN training loss 0.018911\n",
      ">> Epoch 175 finished \tANN training loss 0.018618\n",
      ">> Epoch 176 finished \tANN training loss 0.018331\n",
      ">> Epoch 177 finished \tANN training loss 0.018049\n",
      ">> Epoch 178 finished \tANN training loss 0.017773\n",
      ">> Epoch 179 finished \tANN training loss 0.017503\n",
      ">> Epoch 180 finished \tANN training loss 0.017238\n",
      ">> Epoch 181 finished \tANN training loss 0.016978\n",
      ">> Epoch 182 finished \tANN training loss 0.016723\n",
      ">> Epoch 183 finished \tANN training loss 0.016474\n",
      ">> Epoch 184 finished \tANN training loss 0.016229\n",
      ">> Epoch 185 finished \tANN training loss 0.015989\n",
      ">> Epoch 186 finished \tANN training loss 0.015754\n",
      ">> Epoch 187 finished \tANN training loss 0.015524\n",
      ">> Epoch 188 finished \tANN training loss 0.015298\n",
      ">> Epoch 189 finished \tANN training loss 0.015077\n",
      ">> Epoch 190 finished \tANN training loss 0.014860\n",
      ">> Epoch 191 finished \tANN training loss 0.014647\n",
      ">> Epoch 192 finished \tANN training loss 0.014439\n",
      ">> Epoch 193 finished \tANN training loss 0.014234\n",
      ">> Epoch 194 finished \tANN training loss 0.014034\n",
      ">> Epoch 195 finished \tANN training loss 0.013838\n",
      ">> Epoch 196 finished \tANN training loss 0.013646\n",
      ">> Epoch 197 finished \tANN training loss 0.013457\n",
      ">> Epoch 198 finished \tANN training loss 0.013273\n",
      ">> Epoch 199 finished \tANN training loss 0.013092\n",
      ">> Epoch 200 finished \tANN training loss 0.012914\n",
      ">> Epoch 201 finished \tANN training loss 0.012741\n",
      ">> Epoch 202 finished \tANN training loss 0.012570\n",
      ">> Epoch 203 finished \tANN training loss 0.012403\n",
      ">> Epoch 204 finished \tANN training loss 0.012240\n",
      ">> Epoch 205 finished \tANN training loss 0.012079\n",
      ">> Epoch 206 finished \tANN training loss 0.011922\n",
      ">> Epoch 207 finished \tANN training loss 0.011768\n",
      ">> Epoch 208 finished \tANN training loss 0.011617\n",
      ">> Epoch 209 finished \tANN training loss 0.011469\n",
      ">> Epoch 210 finished \tANN training loss 0.011324\n",
      ">> Epoch 211 finished \tANN training loss 0.011182\n",
      ">> Epoch 212 finished \tANN training loss 0.011043\n",
      ">> Epoch 213 finished \tANN training loss 0.010906\n",
      ">> Epoch 214 finished \tANN training loss 0.010773\n",
      ">> Epoch 215 finished \tANN training loss 0.010642\n",
      ">> Epoch 216 finished \tANN training loss 0.010513\n",
      ">> Epoch 217 finished \tANN training loss 0.010387\n",
      ">> Epoch 218 finished \tANN training loss 0.010264\n",
      ">> Epoch 219 finished \tANN training loss 0.010143\n",
      ">> Epoch 220 finished \tANN training loss 0.010025\n",
      ">> Epoch 221 finished \tANN training loss 0.009909\n",
      ">> Epoch 222 finished \tANN training loss 0.009795\n",
      ">> Epoch 223 finished \tANN training loss 0.009684\n",
      ">> Epoch 224 finished \tANN training loss 0.009574\n",
      ">> Epoch 225 finished \tANN training loss 0.009467\n",
      ">> Epoch 226 finished \tANN training loss 0.009363\n",
      ">> Epoch 227 finished \tANN training loss 0.009260\n",
      ">> Epoch 228 finished \tANN training loss 0.009159\n",
      ">> Epoch 229 finished \tANN training loss 0.009060\n",
      ">> Epoch 230 finished \tANN training loss 0.008964\n",
      ">> Epoch 231 finished \tANN training loss 0.008869\n",
      ">> Epoch 232 finished \tANN training loss 0.008776\n",
      ">> Epoch 233 finished \tANN training loss 0.008685\n",
      ">> Epoch 234 finished \tANN training loss 0.008596\n",
      ">> Epoch 235 finished \tANN training loss 0.008509\n",
      ">> Epoch 236 finished \tANN training loss 0.008423\n",
      ">> Epoch 237 finished \tANN training loss 0.008339\n",
      ">> Epoch 238 finished \tANN training loss 0.008257\n",
      ">> Epoch 239 finished \tANN training loss 0.008177\n",
      ">> Epoch 240 finished \tANN training loss 0.008098\n",
      ">> Epoch 241 finished \tANN training loss 0.008020\n",
      ">> Epoch 242 finished \tANN training loss 0.007945\n",
      ">> Epoch 243 finished \tANN training loss 0.007870\n",
      ">> Epoch 244 finished \tANN training loss 0.007798\n",
      ">> Epoch 245 finished \tANN training loss 0.007726\n",
      ">> Epoch 246 finished \tANN training loss 0.007657\n",
      ">> Epoch 247 finished \tANN training loss 0.007588\n",
      ">> Epoch 248 finished \tANN training loss 0.007521\n",
      ">> Epoch 249 finished \tANN training loss 0.007456\n",
      ">> Epoch 250 finished \tANN training loss 0.007391\n",
      ">> Epoch 251 finished \tANN training loss 0.007328\n",
      ">> Epoch 252 finished \tANN training loss 0.007266\n",
      ">> Epoch 253 finished \tANN training loss 0.007206\n",
      ">> Epoch 254 finished \tANN training loss 0.007146\n",
      ">> Epoch 255 finished \tANN training loss 0.007088\n",
      ">> Epoch 256 finished \tANN training loss 0.007031\n",
      ">> Epoch 257 finished \tANN training loss 0.006976\n",
      ">> Epoch 258 finished \tANN training loss 0.006921\n",
      ">> Epoch 259 finished \tANN training loss 0.006867\n",
      ">> Epoch 260 finished \tANN training loss 0.006815\n",
      ">> Epoch 261 finished \tANN training loss 0.006763\n",
      ">> Epoch 262 finished \tANN training loss 0.006713\n",
      ">> Epoch 263 finished \tANN training loss 0.006664\n",
      ">> Epoch 264 finished \tANN training loss 0.006615\n",
      ">> Epoch 265 finished \tANN training loss 0.006568\n",
      ">> Epoch 266 finished \tANN training loss 0.006522\n",
      ">> Epoch 267 finished \tANN training loss 0.006476\n",
      ">> Epoch 268 finished \tANN training loss 0.006432\n",
      ">> Epoch 269 finished \tANN training loss 0.006388\n",
      ">> Epoch 270 finished \tANN training loss 0.006345\n",
      ">> Epoch 271 finished \tANN training loss 0.006303\n",
      ">> Epoch 272 finished \tANN training loss 0.006262\n",
      ">> Epoch 273 finished \tANN training loss 0.006222\n",
      ">> Epoch 274 finished \tANN training loss 0.006183\n",
      ">> Epoch 275 finished \tANN training loss 0.006144\n",
      ">> Epoch 276 finished \tANN training loss 0.006106\n",
      ">> Epoch 277 finished \tANN training loss 0.006069\n",
      ">> Epoch 278 finished \tANN training loss 0.006033\n",
      ">> Epoch 279 finished \tANN training loss 0.005997\n",
      ">> Epoch 280 finished \tANN training loss 0.005962\n",
      ">> Epoch 281 finished \tANN training loss 0.005928\n",
      ">> Epoch 282 finished \tANN training loss 0.005895\n",
      ">> Epoch 283 finished \tANN training loss 0.005862\n",
      ">> Epoch 284 finished \tANN training loss 0.005830\n",
      ">> Epoch 285 finished \tANN training loss 0.005798\n",
      ">> Epoch 286 finished \tANN training loss 0.005767\n",
      ">> Epoch 287 finished \tANN training loss 0.005737\n",
      ">> Epoch 288 finished \tANN training loss 0.005708\n",
      ">> Epoch 289 finished \tANN training loss 0.005679\n",
      ">> Epoch 290 finished \tANN training loss 0.005650\n",
      ">> Epoch 291 finished \tANN training loss 0.005623\n",
      ">> Epoch 292 finished \tANN training loss 0.005595\n",
      ">> Epoch 293 finished \tANN training loss 0.005569\n",
      ">> Epoch 294 finished \tANN training loss 0.005542\n",
      ">> Epoch 295 finished \tANN training loss 0.005517\n",
      ">> Epoch 296 finished \tANN training loss 0.005492\n",
      ">> Epoch 297 finished \tANN training loss 0.005467\n",
      ">> Epoch 298 finished \tANN training loss 0.005443\n",
      ">> Epoch 299 finished \tANN training loss 0.005419\n",
      ">> Epoch 300 finished \tANN training loss 0.005396\n",
      ">> Epoch 301 finished \tANN training loss 0.005374\n",
      ">> Epoch 302 finished \tANN training loss 0.005351\n",
      ">> Epoch 303 finished \tANN training loss 0.005330\n",
      ">> Epoch 304 finished \tANN training loss 0.005308\n",
      ">> Epoch 305 finished \tANN training loss 0.005288\n",
      ">> Epoch 306 finished \tANN training loss 0.005267\n",
      ">> Epoch 307 finished \tANN training loss 0.005247\n",
      ">> Epoch 308 finished \tANN training loss 0.005227\n",
      ">> Epoch 309 finished \tANN training loss 0.005208\n",
      ">> Epoch 310 finished \tANN training loss 0.005189\n",
      ">> Epoch 311 finished \tANN training loss 0.005171\n",
      ">> Epoch 312 finished \tANN training loss 0.005153\n",
      ">> Epoch 313 finished \tANN training loss 0.005135\n",
      ">> Epoch 314 finished \tANN training loss 0.005118\n",
      ">> Epoch 315 finished \tANN training loss 0.005101\n",
      ">> Epoch 316 finished \tANN training loss 0.005084\n",
      ">> Epoch 317 finished \tANN training loss 0.005068\n",
      ">> Epoch 318 finished \tANN training loss 0.005052\n",
      ">> Epoch 319 finished \tANN training loss 0.005036\n",
      ">> Epoch 320 finished \tANN training loss 0.005021\n",
      ">> Epoch 321 finished \tANN training loss 0.005006\n",
      ">> Epoch 322 finished \tANN training loss 0.004991\n",
      ">> Epoch 323 finished \tANN training loss 0.004977\n",
      ">> Epoch 324 finished \tANN training loss 0.004963\n",
      ">> Epoch 325 finished \tANN training loss 0.004949\n",
      ">> Epoch 326 finished \tANN training loss 0.004935\n",
      ">> Epoch 327 finished \tANN training loss 0.004922\n",
      ">> Epoch 328 finished \tANN training loss 0.004909\n",
      ">> Epoch 329 finished \tANN training loss 0.004896\n",
      ">> Epoch 330 finished \tANN training loss 0.004884\n",
      ">> Epoch 331 finished \tANN training loss 0.004872\n",
      ">> Epoch 332 finished \tANN training loss 0.004860\n",
      ">> Epoch 333 finished \tANN training loss 0.004848\n",
      ">> Epoch 334 finished \tANN training loss 0.004836\n",
      ">> Epoch 335 finished \tANN training loss 0.004825\n",
      ">> Epoch 336 finished \tANN training loss 0.004814\n",
      ">> Epoch 337 finished \tANN training loss 0.004803\n",
      ">> Epoch 338 finished \tANN training loss 0.004793\n",
      ">> Epoch 339 finished \tANN training loss 0.004782\n",
      ">> Epoch 340 finished \tANN training loss 0.004772\n",
      ">> Epoch 341 finished \tANN training loss 0.004762\n",
      ">> Epoch 342 finished \tANN training loss 0.004753\n",
      ">> Epoch 343 finished \tANN training loss 0.004743\n",
      ">> Epoch 344 finished \tANN training loss 0.004734\n",
      ">> Epoch 345 finished \tANN training loss 0.004725\n",
      ">> Epoch 346 finished \tANN training loss 0.004716\n",
      ">> Epoch 347 finished \tANN training loss 0.004707\n",
      ">> Epoch 348 finished \tANN training loss 0.004698\n",
      ">> Epoch 349 finished \tANN training loss 0.004690\n",
      ">> Epoch 350 finished \tANN training loss 0.004681\n",
      ">> Epoch 351 finished \tANN training loss 0.004673\n",
      ">> Epoch 352 finished \tANN training loss 0.004665\n",
      ">> Epoch 353 finished \tANN training loss 0.004658\n",
      ">> Epoch 354 finished \tANN training loss 0.004650\n",
      ">> Epoch 355 finished \tANN training loss 0.004643\n",
      ">> Epoch 356 finished \tANN training loss 0.004635\n",
      ">> Epoch 357 finished \tANN training loss 0.004628\n",
      ">> Epoch 358 finished \tANN training loss 0.004621\n",
      ">> Epoch 359 finished \tANN training loss 0.004614\n",
      ">> Epoch 360 finished \tANN training loss 0.004608\n",
      ">> Epoch 361 finished \tANN training loss 0.004601\n",
      ">> Epoch 362 finished \tANN training loss 0.004595\n",
      ">> Epoch 363 finished \tANN training loss 0.004588\n",
      ">> Epoch 364 finished \tANN training loss 0.004582\n",
      ">> Epoch 365 finished \tANN training loss 0.004576\n",
      ">> Epoch 366 finished \tANN training loss 0.004570\n",
      ">> Epoch 367 finished \tANN training loss 0.004564\n",
      ">> Epoch 368 finished \tANN training loss 0.004558\n",
      ">> Epoch 369 finished \tANN training loss 0.004553\n",
      ">> Epoch 370 finished \tANN training loss 0.004547\n",
      ">> Epoch 371 finished \tANN training loss 0.004542\n",
      ">> Epoch 372 finished \tANN training loss 0.004537\n",
      ">> Epoch 373 finished \tANN training loss 0.004532\n",
      ">> Epoch 374 finished \tANN training loss 0.004527\n",
      ">> Epoch 375 finished \tANN training loss 0.004522\n",
      ">> Epoch 376 finished \tANN training loss 0.004517\n",
      ">> Epoch 377 finished \tANN training loss 0.004512\n",
      ">> Epoch 378 finished \tANN training loss 0.004508\n",
      ">> Epoch 379 finished \tANN training loss 0.004503\n",
      ">> Epoch 380 finished \tANN training loss 0.004499\n",
      ">> Epoch 381 finished \tANN training loss 0.004494\n",
      ">> Epoch 382 finished \tANN training loss 0.004490\n",
      ">> Epoch 383 finished \tANN training loss 0.004486\n",
      ">> Epoch 384 finished \tANN training loss 0.004482\n",
      ">> Epoch 385 finished \tANN training loss 0.004478\n",
      ">> Epoch 386 finished \tANN training loss 0.004474\n",
      ">> Epoch 387 finished \tANN training loss 0.004470\n",
      ">> Epoch 388 finished \tANN training loss 0.004466\n",
      ">> Epoch 389 finished \tANN training loss 0.004462\n",
      ">> Epoch 390 finished \tANN training loss 0.004459\n",
      ">> Epoch 391 finished \tANN training loss 0.004455\n",
      ">> Epoch 392 finished \tANN training loss 0.004452\n",
      ">> Epoch 393 finished \tANN training loss 0.004448\n",
      ">> Epoch 394 finished \tANN training loss 0.004445\n",
      ">> Epoch 395 finished \tANN training loss 0.004442\n",
      ">> Epoch 396 finished \tANN training loss 0.004439\n",
      ">> Epoch 397 finished \tANN training loss 0.004435\n",
      ">> Epoch 398 finished \tANN training loss 0.004432\n",
      ">> Epoch 399 finished \tANN training loss 0.004429\n",
      ">> Epoch 400 finished \tANN training loss 0.004426\n",
      ">> Epoch 401 finished \tANN training loss 0.004424\n",
      ">> Epoch 402 finished \tANN training loss 0.004421\n",
      ">> Epoch 403 finished \tANN training loss 0.004418\n",
      ">> Epoch 404 finished \tANN training loss 0.004415\n",
      ">> Epoch 405 finished \tANN training loss 0.004413\n",
      ">> Epoch 406 finished \tANN training loss 0.004410\n",
      ">> Epoch 407 finished \tANN training loss 0.004407\n",
      ">> Epoch 408 finished \tANN training loss 0.004405\n",
      ">> Epoch 409 finished \tANN training loss 0.004402\n",
      ">> Epoch 410 finished \tANN training loss 0.004400\n",
      ">> Epoch 411 finished \tANN training loss 0.004398\n",
      ">> Epoch 412 finished \tANN training loss 0.004395\n",
      ">> Epoch 413 finished \tANN training loss 0.004393\n",
      ">> Epoch 414 finished \tANN training loss 0.004391\n",
      ">> Epoch 415 finished \tANN training loss 0.004389\n",
      ">> Epoch 416 finished \tANN training loss 0.004387\n",
      ">> Epoch 417 finished \tANN training loss 0.004385\n",
      ">> Epoch 418 finished \tANN training loss 0.004383\n",
      ">> Epoch 419 finished \tANN training loss 0.004381\n",
      ">> Epoch 420 finished \tANN training loss 0.004379\n",
      ">> Epoch 421 finished \tANN training loss 0.004377\n",
      ">> Epoch 422 finished \tANN training loss 0.004375\n",
      ">> Epoch 423 finished \tANN training loss 0.004373\n",
      ">> Epoch 424 finished \tANN training loss 0.004371\n",
      ">> Epoch 425 finished \tANN training loss 0.004369\n",
      ">> Epoch 426 finished \tANN training loss 0.004368\n",
      ">> Epoch 427 finished \tANN training loss 0.004366\n",
      ">> Epoch 428 finished \tANN training loss 0.004364\n",
      ">> Epoch 429 finished \tANN training loss 0.004363\n",
      ">> Epoch 430 finished \tANN training loss 0.004361\n",
      ">> Epoch 431 finished \tANN training loss 0.004360\n",
      ">> Epoch 432 finished \tANN training loss 0.004358\n",
      ">> Epoch 433 finished \tANN training loss 0.004357\n",
      ">> Epoch 434 finished \tANN training loss 0.004355\n",
      ">> Epoch 435 finished \tANN training loss 0.004354\n",
      ">> Epoch 436 finished \tANN training loss 0.004352\n",
      ">> Epoch 437 finished \tANN training loss 0.004351\n",
      ">> Epoch 438 finished \tANN training loss 0.004350\n",
      ">> Epoch 439 finished \tANN training loss 0.004348\n",
      ">> Epoch 440 finished \tANN training loss 0.004347\n",
      ">> Epoch 441 finished \tANN training loss 0.004346\n",
      ">> Epoch 442 finished \tANN training loss 0.004345\n",
      ">> Epoch 443 finished \tANN training loss 0.004343\n",
      ">> Epoch 444 finished \tANN training loss 0.004342\n",
      ">> Epoch 445 finished \tANN training loss 0.004341\n",
      ">> Epoch 446 finished \tANN training loss 0.004340\n",
      ">> Epoch 447 finished \tANN training loss 0.004339\n",
      ">> Epoch 448 finished \tANN training loss 0.004338\n",
      ">> Epoch 449 finished \tANN training loss 0.004337\n",
      ">> Epoch 450 finished \tANN training loss 0.004336\n",
      ">> Epoch 451 finished \tANN training loss 0.004334\n",
      ">> Epoch 452 finished \tANN training loss 0.004333\n",
      ">> Epoch 453 finished \tANN training loss 0.004332\n",
      ">> Epoch 454 finished \tANN training loss 0.004332\n",
      ">> Epoch 455 finished \tANN training loss 0.004331\n",
      ">> Epoch 456 finished \tANN training loss 0.004330\n",
      ">> Epoch 457 finished \tANN training loss 0.004329\n",
      ">> Epoch 458 finished \tANN training loss 0.004328\n",
      ">> Epoch 459 finished \tANN training loss 0.004327\n",
      ">> Epoch 460 finished \tANN training loss 0.004326\n",
      ">> Epoch 461 finished \tANN training loss 0.004325\n",
      ">> Epoch 462 finished \tANN training loss 0.004324\n",
      ">> Epoch 463 finished \tANN training loss 0.004324\n",
      ">> Epoch 464 finished \tANN training loss 0.004323\n",
      ">> Epoch 465 finished \tANN training loss 0.004322\n",
      "Training stopped at epoch: 465 because of no improvement\n",
      "Best ANN training loss: 0.004330585710704327\n",
      "[END] Fine tuning step\n",
      "Training time:  709.358583688736\n",
      "Model has been saved: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001\n",
      "Testing model: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001...\n",
      "Test Image: peppers_256.png\n",
      "Reference Image: peppers.tif\n",
      "PSNR 1 Interpolated: 31.096 dB\n",
      "SSIM 1 Interpolated: 80.335 %\n",
      "RMSE 1 Interpolated: 0.08939135479425143\n",
      "PSNR 1: 30.688 dB\n",
      "SSIM 1: 79.189 %\n",
      "RMSE 1: 0.08939135479425143\n",
      "Testing model: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001...\n",
      "Test Image: venus_256.png\n",
      "Reference Image: venus.bmp\n",
      "PSNR 2 Interpolated: 27.197 dB\n",
      "SSIM 2 Interpolated: 82.620 %\n",
      "RMSE 2 Interpolated: 0.08907645204038789\n",
      "PSNR 2: 26.765 dB\n",
      "SSIM 2: 80.628 %\n",
      "RMSE 2: 0.08907645204038789\n",
      "Testing model: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001...\n",
      "Test Image: tiffany_256.png\n",
      "Reference Image: tiffany.bmp\n",
      "PSNR 3 Interpolated: 30.764 dB\n",
      "SSIM 3 Interpolated: 89.466 %\n",
      "RMSE 3 Interpolated: 0.08469977097850741\n",
      "PSNR 3: 30.488 dB\n",
      "SSIM 3: 88.625 %\n",
      "RMSE 3: 0.08469977097850741\n",
      "Testing model: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001...\n",
      "Test Image: brickwall_256.png\n",
      "Reference Image: brickwall.bmp\n",
      "PSNR 4 Interpolated: 29.869 dB\n",
      "SSIM 4 Interpolated: 88.099 %\n",
      "RMSE 4 Interpolated: 0.08930577121241065\n",
      "PSNR 4: 29.428 dB\n",
      "SSIM 4: 86.985 %\n",
      "RMSE 4: 0.08930577121241065\n",
      "Testing model: model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001...\n",
      "Test Image: airplane_256.png\n",
      "Reference Image: airplane.bmp\n",
      "PSNR 5 Interpolated: 31.342 dB\n",
      "SSIM 5 Interpolated: 94.150 %\n",
      "RMSE 5 Interpolated: 0.06317037944867038\n",
      "PSNR 5: 31.026 dB\n",
      "SSIM 5: 93.593 %\n",
      "RMSE 5: 0.06317037944867038\n",
      "Finished processing row 52/52 with model model_mandrill.tif_f(airplane_256.png)_512_x2_p16_s4_l[512, 768, 768, 512]_sigmoid_lr0.01_lrft0.001\n"
     ]
    }
   ],
   "source": [
    "def main(idx, row):\n",
    "    try:\n",
    "        # Baca parameter dari Excel untuk baris ini\n",
    "        train = row[\"train\"]\n",
    "        fine_image = row[\"fine_tuning\"]\n",
    "        fine_label = row[\"label\"]\n",
    "\n",
    "        train_resolution = row['train_res']\n",
    "\n",
    "        interpolation_factor = row['factor']\n",
    "        patch_size = (row['patch_size'], row['patch_size'])\n",
    "        stride = (row['stride'], row['stride'])\n",
    "        data_train_size = row[\"data_train_size\"]\n",
    "\n",
    "        lr = row[\"lr\"]\n",
    "        lr_ft = row[\"lr_ft\"]\n",
    "        epoch = 500\n",
    "        epoch_fine = 500\n",
    "        layers = [int(layer) for layer in str(row['layers']).split(\",\")]\n",
    "        batch_size = row['batch_size']\n",
    "        activation_function = row[\"activation_function\"]\n",
    "\n",
    "        model_name = f\"model_{train}_f({fine_image})_{train_resolution}_x{interpolation_factor}_p{patch_size[0]}_s{stride[0]}_l{layers}_{activation_function}_lr{lr}_lrft{lr_ft}\"\n",
    "        print(f\"\\nTraining model: {model_name}\")\n",
    "\n",
    "        # #### Model Configuration ####\n",
    "        dbn = SupervisedDBNRegression(\n",
    "                    hidden_layers_structure=layers,\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate_rbm=lr,\n",
    "                    n_epochs_rbm=epoch,\n",
    "                    activation_function=activation_function,\n",
    "                    optimization_algorithm='sgd',\n",
    "                    learning_rate=lr_ft,\n",
    "                    n_iter_backprop=epoch_fine,\n",
    "        )\n",
    "\n",
    "        #### Load Data and Preprocess ####\n",
    "        dp = DataProcessing()\n",
    "\n",
    "        training_image = load_image(f\"train/{train}\")\n",
    "        # visualize_image(training_image, \"Pre-Training Image\")\n",
    "        train_patches = dp.get_patches(training_image, patch_size=patch_size, stride=stride)\n",
    "        norm, _ = dp.normalize_for_rbm(train_patches[:train_patches.shape[0]])\n",
    "        if data_train_size < train_patches.shape[0]:\n",
    "            norm = dp.normalize_for_rbm(train_patches[:data_train_size])\n",
    "\n",
    "        X_pretrain = dp.preprocess_for_rbm(norm)\n",
    "\n",
    "        del train_patches, training_image, norm\n",
    "        gc.collect()\n",
    "\n",
    "        fine_image = dp.interpolate(load_image(f'train/{fine_image}'), 2)\n",
    "        label = load_image(f'train/{fine_label}')\n",
    "\n",
    "        # visualize_histogram_compare(fine_image, label, \"Interpolated Fine Tuning Image\", \"Label Image\")\n",
    "\n",
    "        fine_patches = dp.get_patches(fine_image, patch_size=patch_size, stride=stride)\n",
    "        norm, _ = dp.normalize_for_rbm(fine_patches[:fine_patches.shape[0]])\n",
    "        if data_train_size < fine_patches.shape[0]:\n",
    "            norm = dp.normalize_for_rbm(fine_patches[:data_train_size])\n",
    "        X = dp.preprocess_for_rbm(norm)\n",
    "\n",
    "        del fine_patches, fine_image, norm\n",
    "        gc.collect()\n",
    "\n",
    "        label_patches = dp.get_patches(label, patch_size=patch_size, stride=stride)\n",
    "        norm, _ = dp.normalize_for_rbm(label_patches[:label_patches.shape[0]])\n",
    "        if data_train_size < label_patches.shape[0]:\n",
    "            norm = dp.normalize_for_rbm(label_patches[:data_train_size])\n",
    "        y = dp.preprocess_for_rbm(norm)\n",
    "\n",
    "        del label, label_patches, norm\n",
    "        gc.collect()\n",
    "\n",
    "        #### Train the Model ####\n",
    "        if not(os.path.exists(f\"model/{model_name}.h5\")):\n",
    "            print(f\"Starting training for model: {model_name}\")\n",
    "            start_time = time.time()\n",
    "            dbn.fit(X_pretrain,X,y)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"Training time: \", (end_time-start_time))\n",
    "            dbn.save(f\"model/{model_name}.h5\")\n",
    "\n",
    "            print(f\"Model has been saved: {model_name}\")\n",
    "            exp_df.at[idx, \"training_time\"] = (end_time-start_time)\n",
    "\n",
    "            del X_pretrain, X, y\n",
    "            gc.collect()\n",
    "        \n",
    "        else:\n",
    "            print(\"Model alrady exist!\")\n",
    "\n",
    "        #### Testing and Saving Results ####\n",
    "        for i in range(1, 6):\n",
    "            psnr_baseline, ssim_baseline, psnr, ssim,rmse = test(i, row, model_name, dbn, interpolation_factor, patch_size,stride,data_train_size, dp)\n",
    "            # Save results to DataFrame\n",
    "            exp_df.at[idx, f\"b_psnr{i}\"] = psnr_baseline\n",
    "            exp_df.at[idx, f\"b_ssim{i}\"] = ssim_baseline\n",
    "            exp_df.at[idx, f\"psnr{i}\"] = psnr\n",
    "            exp_df.at[idx, f\"ssim{i}\"] = ssim\n",
    "            exp_df.at[idx, f\"rmse{i}\"] = rmse\n",
    "            exp_df.to_excel(excel_name, index=False)\n",
    "\n",
    "        print(f\"Finished processing row {idx+1}/{len(exp_df)} with model {model_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx+1}: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Ensure all variables are deleted in the end\n",
    "        del train, fine_label, train_resolution, interpolation_factor\n",
    "        del patch_size, stride, data_train_size, lr, epoch, layers, batch_size, activation_function, model_name\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "specific_index = 51\n",
    "\n",
    "# Process the specific row\n",
    "if specific_index in exp_df.index:\n",
    "    row = exp_df.loc[specific_index]\n",
    "    main(specific_index, row) \n",
    "else:\n",
    "    print(f\"Index {specific_index} not found in the DataFrame.\")\n",
    "\n",
    "\n",
    "# for idx, row in tqdm(exp_df.iterrows(), total=len(exp_df), desc=\"Processing Rows\"):\n",
    "#     main(idx,row)\n",
    "\n",
    "\n",
    "# print(\"All rows processed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PSNR:  48\n",
      "Best SSIM:  50\n",
      "Best RMSE:  50\n",
      "Index 48 PSNR: 29.69913776549264\n",
      "Index 48 SSIM: 85.789677163208\n",
      "Index 48 RMSE: 0.08252284693938089\n",
      "Index 50 PSNR: 29.697136100181417\n",
      "Index 50 SSIM: 85.84717140590854\n",
      "Index 50 RMSE: 0.0814795988890972\n",
      "train                       mandrill.tif\n",
      "fine_tuning            brickwall_256.png\n",
      "label                      brickwall.bmp\n",
      "test1                    peppers_256.png\n",
      "test1_ref                    peppers.tif\n",
      "test2                      venus_256.png\n",
      "test2_ref                      venus.bmp\n",
      "test3                    tiffany_256.png\n",
      "test3_ref                    tiffany.bmp\n",
      "test4                  brickwall_256.png\n",
      "test4_ref                  brickwall.bmp\n",
      "test5                   airplane_256.png\n",
      "test5_ref                   airplane.bmp\n",
      "train_res                            512\n",
      "test_res                             256\n",
      "factor                                 2\n",
      "patch_size                            16\n",
      "stride                                 4\n",
      "data_train_size                    15625\n",
      "layers                   512,768,768,512\n",
      "batch_size                           512\n",
      "activation_function              sigmoid\n",
      "lr                                  0.01\n",
      "lr_ft                              0.001\n",
      "b_psnr1                          31.0964\n",
      "b_ssim1                          80.3353\n",
      "psnr1                            30.6981\n",
      "ssim1                            79.2375\n",
      "rmse1                          0.0859661\n",
      "b_psnr2                          27.1971\n",
      "b_ssim2                          82.6198\n",
      "psnr2                            26.7662\n",
      "ssim2                             80.672\n",
      "rmse2                          0.0842271\n",
      "b_psnr3                          30.7637\n",
      "b_ssim3                          89.4656\n",
      "psnr3                            30.5204\n",
      "ssim3                            88.6646\n",
      "rmse3                          0.0824118\n",
      "b_psnr4                          29.8692\n",
      "b_ssim4                          88.0986\n",
      "psnr4                            29.4276\n",
      "ssim4                            86.9789\n",
      "rmse4                          0.0766927\n",
      "b_psnr5                          31.3422\n",
      "b_ssim5                          94.1504\n",
      "psnr5                            31.0734\n",
      "ssim5                            93.6829\n",
      "rmse5                          0.0781004\n",
      "training_time                    272.771\n",
      "psnr_average                     29.6971\n",
      "ssim_average                     85.8472\n",
      "rmse_average                   0.0814796\n",
      "Name: 50, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ft_df = exp_df.iloc[45:51]\n",
    "ft_df[\"lr\"]\n",
    "\n",
    "exp_df['psnr_average'] = ft_df[['psnr1', 'psnr2', 'psnr3', 'psnr4', 'psnr5']].mean(axis=1)\n",
    "exp_df['ssim_average'] = ft_df[['ssim1', 'ssim2', 'ssim3', 'ssim4', 'ssim5']].mean(axis=1)\n",
    "exp_df['rmse_average'] = ft_df[['rmse1', 'rmse2', 'rmse3', 'rmse4', 'rmse5']].mean(axis=1)\n",
    "\n",
    "print(\"Best PSNR: \", exp_df['psnr_average'].idxmax())\n",
    "print(\"Best SSIM: \", exp_df['ssim_average'].idxmax())\n",
    "print(\"Best RMSE: \", exp_df['rmse_average'].idxmin())\n",
    "\n",
    "print(\"Index 48 PSNR:\", exp_df.iloc[48]['psnr_average'])\n",
    "print(\"Index 48 SSIM:\", exp_df.iloc[48]['ssim_average'])\n",
    "print(\"Index 48 RMSE:\", exp_df.iloc[48]['rmse_average'])\n",
    "print(\"Index 50 PSNR:\", exp_df.iloc[50]['psnr_average'])\n",
    "print(\"Index 50 SSIM:\", exp_df.iloc[50]['ssim_average'])\n",
    "print(\"Index 50 RMSE:\", exp_df.iloc[50]['rmse_average'])\n",
    "\n",
    "print(exp_df.iloc[50])\n",
    "\n",
    "### Best fine tuning: Brickwall ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrft_df = exp_df.iloc[41:45]\n",
    "# lrft_df[\"lr\"]\n",
    "\n",
    "# exp_df['psnr_average'] = lrft_df[['psnr1', 'psnr2', 'psnr3', 'psnr4', 'psnr5']].mean(axis=1)\n",
    "# exp_df['ssim_average'] = lrft_df[['ssim1', 'ssim2', 'ssim3', 'ssim4', 'ssim5']].mean(axis=1)\n",
    "# exp_df['rmse_average'] = lrft_df[['rmse1', 'rmse2', 'rmse3', 'rmse4', 'rmse5']].mean(axis=1)\n",
    "\n",
    "# print(\"Best PSNR: \", exp_df['psnr_average'].idxmax())\n",
    "# print(\"Best SSIM: \", exp_df['ssim_average'].idxmax())\n",
    "# print(\"Best RMSE: \", exp_df['rmse_average'].idxmin())\n",
    "\n",
    "# print(\"Index 41 PSNR:\", exp_df.iloc[41]['psnr_average'])\n",
    "# print(\"Index 41 SSIM:\", exp_df.iloc[41]['ssim_average'])\n",
    "# print(\"Index 41 RMSE:\", exp_df.iloc[41]['rmse_average'])\n",
    "# print(\"Index 42 PSNR:\", exp_df.iloc[42]['psnr_average'])\n",
    "# print(\"Index 42 SSIM:\", exp_df.iloc[42]['ssim_average'])\n",
    "# print(\"Index 42 RMSE:\", exp_df.iloc[42]['rmse_average'])\n",
    "\n",
    "# print(exp_df.iloc[42])\n",
    "\n",
    "### Best Learning Rate Fine Tuning: 0.001 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_df = exp_df.iloc[37:41]\n",
    "# lr_df[\"lr\"]\n",
    "\n",
    "# exp_df['psnr_average'] = lr_df[['psnr1', 'psnr2', 'psnr3', 'psnr4', 'psnr5']].mean(axis=1)\n",
    "# exp_df['ssim_average'] = lr_df[['ssim1', 'ssim2', 'ssim3', 'ssim4', 'ssim5']].mean(axis=1)\n",
    "# exp_df['rmse_average'] = lr_df[['rmse1', 'rmse2', 'rmse3', 'rmse4', 'rmse5']].mean(axis=1)\n",
    "\n",
    "# print(\"Best PSNR: \", exp_df['psnr_average'].idxmax())\n",
    "# print(\"Best SSIM: \", exp_df['ssim_average'].idxmax())\n",
    "# print(\"Best RMSE: \", exp_df['rmse_average'].idxmin())\n",
    "\n",
    "# print(exp_df.iloc[38])\n",
    "# ### BEST LEARNING RATE 0.01 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_df = exp_df.iloc[33:37]\n",
    "# a_df[\"activation_function\"]\n",
    "\n",
    "# exp_df['psnr_average'] = a_df[['psnr1', 'psnr2', 'psnr3', 'psnr4', 'psnr5']].mean(axis=1)\n",
    "# exp_df['ssim_average'] = a_df[['ssim1', 'ssim2', 'ssim3', 'ssim4', 'ssim5']].mean(axis=1)\n",
    "# exp_df['rmse_average'] = a_df[['rmse1', 'rmse2', 'rmse3', 'rmse4', 'rmse5']].mean(axis=1)\n",
    "\n",
    "# print(\"Best PSNR: \", exp_df['psnr_average'].idxmax())\n",
    "# print(\"Best SSIM: \", exp_df['ssim_average'].idxmax())\n",
    "# print(\"Best RMSE: \", exp_df['rmse_average'].idxmin())\n",
    "\n",
    "# print(exp_df.iloc[26])\n",
    "\n",
    "### Best Activation Function: SIGMOID ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps_df = exp_df.iloc[23:33]\n",
    "# exp_df['psnr_average'] = ps_df[['psnr1', 'psnr2', 'psnr3', 'psnr4', 'psnr5']].mean(axis=1)\n",
    "# exp_df['ssim_average'] = ps_df[['ssim1', 'ssim2', 'ssim3', 'ssim4', 'ssim5']].mean(axis=1)\n",
    "# exp_df['rmse_average'] = ps_df[['rmse1', 'rmse2', 'rmse3', 'rmse4', 'rmse5']].mean(axis=1)\n",
    "\n",
    "# print(\"Best PSNR: \", exp_df['psnr_average'].idxmax())\n",
    "# print(\"Best SSIM: \", exp_df['ssim_average'].idxmax())\n",
    "# print(\"Best RMSE: \", exp_df['rmse_average'].idxmin())\n",
    "\n",
    "# print(exp_df.iloc[26])\n",
    "\n",
    "# ### BEST PATCH (16x16), STRIDE (4x4) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"PSNR:\")\n",
    "# best_psnr1_index = exp_df['psnr1'].idxmax()\n",
    "# print(best_psnr1_index)\n",
    "# best_psnr2_index = exp_df['psnr2'].idxmax()\n",
    "# print(best_psnr2_index)\n",
    "# best_psnr3_index = exp_df['psnr3'].idxmax()\n",
    "# print(best_psnr3_index)\n",
    "# best_psnr4_index = exp_df['psnr4'].idxmax()\n",
    "# print(best_psnr4_index)\n",
    "# best_psnr5_index = exp_df['psnr5'].idxmax()\n",
    "# print(best_psnr5_index)\n",
    "\n",
    "# print(\"SSIM:\")\n",
    "# best_ssim1_index = exp_df['ssim1'].idxmax()\n",
    "# print(best_ssim1_index)\n",
    "# best_ssim2_index = exp_df['ssim2'].idxmax()\n",
    "# print(best_ssim2_index)\n",
    "# best_ssim3_index = exp_df['ssim3'].idxmax()\n",
    "# print(best_ssim3_index)\n",
    "# best_ssim4_index = exp_df['ssim4'].idxmax()\n",
    "# print(best_ssim4_index)\n",
    "# best_ssim5_index = exp_df['ssim5'].idxmax()\n",
    "# print(best_ssim5_index)\n",
    "\n",
    "# print(\"RMSE:\")\n",
    "# best_rmse1_index = exp_df['rmse1'].idxmin()\n",
    "# print(best_rmse1_index)\n",
    "# best_rmse2_index = exp_df['rmse2'].idxmin()\n",
    "# print(best_rmse2_index)\n",
    "# best_rmse3_index = exp_df['rmse3'].idxmin()\n",
    "# print(best_rmse3_index)\n",
    "# best_rmse4_index = exp_df['rmse4'].idxmin()\n",
    "# print(best_rmse4_index)\n",
    "# best_rmse5_index = exp_df['rmse5'].idxmin()\n",
    "# print(best_rmse5_index)\n",
    "\n",
    "\n",
    "# exp_df['psnr_average'] = exp_df[['psnr1', 'psnr2', 'psnr3', 'psnr4', 'psnr5']].mean(axis=1)\n",
    "# exp_df['ssim_average'] = exp_df[['ssim1', 'ssim2', 'ssim3', 'ssim4', 'ssim5']].mean(axis=1)\n",
    "# exp_df['rmse_average'] = exp_df[['rmse1', 'rmse2', 'rmse3', 'rmse4', 'rmse5']].mean(axis=1)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(exp_df.index, exp_df['psnr_average'], marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('PSNR Average')\n",
    "# plt.title('PSNR Average over Index')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(exp_df.index, exp_df['ssim_average'], marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('SSIM Average')\n",
    "# plt.title('SSIM Average over Index')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(exp_df.index, exp_df['rmse_average'], marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('RMSE Average')\n",
    "# plt.title('RMSE Average over Index')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Best PSNR Average\", exp_df['psnr_average'].idxmax())\n",
    "# print(\"Best SSIM Average\", exp_df['ssim_average'].idxmax())\n",
    "# print(\"Best RMSE Average\", exp_df['rmse_average'].idxmin())\n",
    "\n",
    "\n",
    "# exp_df['total_average'] = exp_df['psnr_average'] + exp_df['ssim_average'] - (exp_df['rmse_average'] * 100)\n",
    "\n",
    "# print(exp_df['total_average'].idxmax())\n",
    "\n",
    "# print(\"Index 4 PSNR:\", exp_df.iloc[4]['psnr_average'])\n",
    "# print(\"Index 4 SSIM:\", exp_df.iloc[4]['ssim_average'])\n",
    "# print(\"Index 4 RMSE:\", exp_df.iloc[4]['rmse_average'])\n",
    "# print(\"Index 12 PSNR:\", exp_df.iloc[12]['psnr_average'])\n",
    "# print(\"Index 12 SSIM:\", exp_df.iloc[12]['ssim_average'])\n",
    "# print(\"Index 12 RMSE:\", exp_df.iloc[12]['rmse_average'])\n",
    "# print(\"Index 15 PSNR:\", exp_df.iloc[15]['psnr_average'])\n",
    "# print(\"Index 15 SSIM:\", exp_df.iloc[15]['ssim_average'])\n",
    "# print(\"Index 15 RMSE:\", exp_df.iloc[15]['rmse_average'])\n",
    "\n",
    "### INDEX 15 SSIM paling tinggi, psnr rmse not bad ###\n",
    "### Best Layers: 512,768,768,512 ###\n",
    "# print(exp_df.iloc[15][\"layers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Training DBN\n",
    "# def train(model, input : np.ndarray, model_name : str, repetitions: int, beta:float, interpolation_factor:float):\n",
    "#     dp = DataProcessing()\n",
    "\n",
    "#     visualize_histogram(input, title=f\"Training Image\")\n",
    "\n",
    "#     curr_rep = 1\n",
    "#     print(\"Repetisi saat ini: \", curr_rep)\n",
    "\n",
    "#     model.fit(input)\n",
    "#     model.save(f\"model/{model_name}_{curr_rep}.h5\")\n",
    "\n",
    "#     while curr_rep < repetitions:\n",
    "#         model = model.load(f\"model/{model_name}_{curr_rep}.h5\")\n",
    "#         r = model.transform(input)\n",
    "#         print(\"Shape Transformed: \", r.shape)\n",
    "#         input = dp.proccess_output(u=input, r=r, beta=beta, s=interpolation_factor)\n",
    "#         visualize_histogram(input, title=f\"Training Image: After transform ({curr_rep})\")\n",
    "#         r = None\n",
    "\n",
    "#         curr_rep += 1\n",
    "#         print(\"Repetisi saat ini: \", curr_rep )\n",
    "\n",
    "#         model.fit(input)\n",
    "#         model.save(f\"model/{model_name}_{curr_rep}.h5\")\n",
    "\n",
    "# ## Test DBN\n",
    "# def test(test_image, test_reference_image, model, patch_size:tuple, stride:tuple, interpolation_factor:int):\n",
    "#     dp = DataProcessing()\n",
    "\n",
    "#     desired_shape = test_reference_image.shape\n",
    "    \n",
    "#     test_image = dp.interpolate(test_image, interpolation_factor=interpolation_factor)\n",
    "    \n",
    "#     visualize_image(test_image, title=\"Test Image after interpolation\")\n",
    "#     visualize_histogram(test_image, title=\"Test Image after interpolation\", range=(0,256))\n",
    "#     psnr_baseline_value = calculate_psnr(test_reference_image, test_image)\n",
    "#     ssim_baseline_value = calculate_ssim(test_reference_image, test_image)*100\n",
    "\n",
    "#     psnr_print = f\"PSNR Baseline value: {psnr_baseline_value:,.3f} dB\"\n",
    "#     ssim_print = f\"SSIM Baseline value: {ssim_baseline_value:,.3f}%\"\n",
    "#     print(psnr_print.replace(\".\", \",\"))\n",
    "#     print(ssim_print.replace(\".\", \",\"))\n",
    "\n",
    "#     test_patches = dp.get_patches(test_image,patch_size=patch_size, stride=stride)\n",
    "\n",
    "#     # Process data for rbm\n",
    "#     test_patches = dp.preprocess_for_rbm(test_patches)\n",
    "\n",
    "#     # Infer test to model\n",
    "#     result = model.transform(test_patches)\n",
    "\n",
    "#     test_patches = dp.inverse_preprocess(\n",
    "#         dp.proccess_output(test_patches, result, 1, interpolation_factor),\n",
    "#         original_patch_shape=(patch_size[0], patch_size[1], 3)\n",
    "#         )\n",
    "    \n",
    "#     result = None\n",
    "\n",
    "#     # visualize_patches(test_patches, title=\"Test Patches Example\", visualize_size=(6,6))\n",
    "\n",
    "#     reconstruct_image = dp.reconstruct_from_patches(test_patches, original_shape=desired_shape, patch_size=patch_size, stride=stride)\n",
    "\n",
    "#     test_patches = None\n",
    "\n",
    "#     visualize_histogram_compare(original_image=test_reference_image, reconstruct_image=reconstruct_image)\n",
    "#     psnr_value = calculate_psnr(test_reference_image, reconstruct_image)\n",
    "#     ssim_value = calculate_ssim(test_reference_image, reconstruct_image)*100\n",
    "\n",
    "#     psnr_print = f\"PSNR value: {psnr_value:,.3f} dB\"\n",
    "#     ssim_print = f\"SSIM value: {ssim_value:,.3f}%\"\n",
    "\n",
    "#     print(psnr_print.replace(\".\", \",\"))\n",
    "#     print(ssim_print.replace(\".\", \",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHb0fnnI9sJm",
    "outputId": "a0cfcd81-0c0b-4a1e-da29-7e395f8b64d8"
   },
   "outputs": [],
   "source": [
    "# tracemalloc.start()\n",
    "\n",
    "# #### Patch size and Stride ####\n",
    "# patch_size = (16,16)\n",
    "# stride = (4,4)\n",
    "\n",
    "# #### Load Data ####\n",
    "# train_resolution = 512\n",
    "# test_resolution = 256\n",
    "\n",
    "# dp = DataProcessing()\n",
    "\n",
    "# # Load Pre Training Data\n",
    "# training_image = load_image(\"train/peppers.png\")\n",
    "# visualize_image(training_image, \"Training Image\")\n",
    "\n",
    "# train_patches = dp.get_patches(training_image, patch_size=patch_size, stride=stride)\n",
    "\n",
    "# # Feel free to reduce training set\n",
    "# training_set_num = train_patches.shape[0]\n",
    "\n",
    "# # visualize_patches(train_patches, \"Train patches example\")\n",
    "\n",
    "\n",
    "# X_train = dp.preprocess_for_rbm(train_patches[:training_set_num])\n",
    "\n",
    "# train_patches = None\n",
    "# training_image = None\n",
    "\n",
    "# # Load Fine Tuning Data\n",
    "# fine_image = load_image(\"test/lenna_256.png\")\n",
    "# label_fine = load_image(\"train/lenna.png\")\n",
    "# fine_image = dp.interpolate(fine_image, 2)\n",
    "\n",
    "# fine_patches = dp.get_patches(fine_image, patch_size, stride)\n",
    "# label_patches = dp.get_patches(label_fine, patch_size, stride)\n",
    "\n",
    "# fine_train = dp.preprocess_for_rbm(fine_patches)\n",
    "# label_train = dp.preprocess_for_rbm(label_patches)\n",
    "\n",
    "\n",
    "\n",
    "# #### Training parameter ###\n",
    "# interpolation_factor = 2\n",
    "# beta = 1\n",
    "# Repetitions = 1\n",
    "# lr = 0.01\n",
    "# epoch = 100\n",
    "\n",
    "# layers = [400,200,768]\n",
    "# batch_size = 1024\n",
    "# activation_function = 'relu'\n",
    "\n",
    "# model_name = f\"model_peppers_{train_resolution}_x{interpolation_factor}_p{patch_size[0]}_s{stride[0]}_({layers[0]}_{layers[1]}_{layers[2]})\"\n",
    "\n",
    "# # Models we will use\n",
    "# # dbn = UnsupervisedDBN(hidden_layers_structure=layers,\n",
    "# #                       batch_size=batch_size,\n",
    "# #                       learning_rate_rbm=lr,\n",
    "# #                       n_epochs_rbm=epoch,\n",
    "# #                       activation_function=activation_function,\n",
    "# #                       optimization_algorithm='sgd',)\n",
    "\n",
    "# dbn = SupervisedDBNRegression(\n",
    "#                     hidden_layers_structure=layers,\n",
    "#                       batch_size=batch_size,\n",
    "#                       learning_rate_rbm=lr,\n",
    "#                       n_epochs_rbm=epoch,\n",
    "#                       activation_function=activation_function,\n",
    "#                       optimization_algorithm='sgd',\n",
    "#                                     learning_rate=lr,\n",
    "#                                     n_iter_backprop=200,\n",
    "\n",
    "# )\n",
    "\n",
    "# dbn.fit(X_train, fine_train, label_train)\n",
    "# dbn.save(\"model/hmmm.h5\")\n",
    "# #### Train and Test ####\n",
    "\n",
    "# snapshot1 = tracemalloc.take_snapshot()\n",
    "\n",
    "# # comment this to skip training\n",
    "# # train(\n",
    "# #     model=dbn, \n",
    "# #     input=X_train,\n",
    "# #     model_name=model_name,\n",
    "# #     beta=beta,\n",
    "# #     interpolation_factor=interpolation_factor,\n",
    "# #     repetitions=Repetitions, \n",
    "# # )\n",
    "\n",
    "# snapshot2 = tracemalloc.take_snapshot()\n",
    "# top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n",
    "\n",
    "# print(\"[ Top 10 differences in memory allocation ]\")\n",
    "# for stat in top_stats[:10]:\n",
    "#     print(stat)\n",
    "\n",
    "# # Load Testing Data\n",
    "# print(\"== Test 1 ==\")\n",
    "# test_image = load_image(f\"test/peppers_256.png\")\n",
    "# visualize_image(test_image, title=\"Test Image\")\n",
    "\n",
    "# test_reference_image = load_image(f\"train/peppers.png\")\n",
    "# dbn = dbn.load(f\"model/hmmm.h5\")\n",
    "\n",
    "# test(\n",
    "#     test_image=test_image,\n",
    "#     test_reference_image=test_reference_image,\n",
    "#     model=dbn,\n",
    "#     patch_size=patch_size,\n",
    "#     stride=stride,\n",
    "#     interpolation_factor=interpolation_factor\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# print(\"== Test 2 ==\")\n",
    "# test_image = load_image(f\"test/lenna_256.png\")\n",
    "# visualize_image(test_image, title=\"Test Image\")\n",
    "\n",
    "# test_reference_image = load_image(f\"train/lenna.png\")\n",
    "# dbn = dbn.load(f\"model/{model_name}_{Repetitions}.h5\")\n",
    "\n",
    "# test(\n",
    "#     test_image=test_image,\n",
    "#     test_reference_image=test_reference_image,\n",
    "#     model=dbn,\n",
    "#     patch_size=patch_size,\n",
    "#     stride=stride,\n",
    "#     interpolation_factor=interpolation_factor\n",
    "# )\n",
    "\n",
    "# print(\"== Test 3 ==\")\n",
    "# test_image = load_image(f\"test/{test_resolution}/ct_lung4_{test_resolution}.png\")\n",
    "# visualize_image(test_image, title=\"Test Image\")\n",
    "\n",
    "# test_reference_image = load_image(f\"test/{train_resolution}/ct_lung4_{train_resolution}.png\")\n",
    "# dbn = dbn.load(f\"model/{model_name}_{Repetitions}.h5\")\n",
    "\n",
    "# test(\n",
    "#     test_image=test_image,\n",
    "#     test_reference_image=test_reference_image,\n",
    "#     model=dbn,\n",
    "#     patch_size=patch_size,\n",
    "#     stride=stride,\n",
    "#     interpolation_factor=interpolation_factor\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI5OV_F2Rcgw"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO78JLF4XsGcABgpg/L012g",
   "collapsed_sections": [
    "9PJwKQlYOFWo",
    "ilnhcVI9OKMl",
    "97TIoNNAOO2M",
    "urgQvFpuOVcn",
    "7Hub2KYPOYH1"
   ],
   "include_colab_link": true,
   "mount_file_id": "1StnRFjOmR44zwENLuo85hX_x__mgEbKS",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
